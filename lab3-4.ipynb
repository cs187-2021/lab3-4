{"cells": [{"cell_type": "code", "execution_count": null, "id": "0209f260", "metadata": {"deletable": false, "editable": false, "jupyter": {"outputs_hidden": true, "source_hidden": true}}, "outputs": [], "source": ["# Please do not change this cell because some hidden tests might depend on it.\n", "import os\n", "\n", "# Otter grader does not handle ! commands well, so we define and use our\n", "# own function to execute shell commands.\n", "def shell(commands, warn=True):\n", "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n", "     \n", "       Prints the result to stdout and returns the exit status. \n", "       Provides a printed warning on non-zero exit status unless `warn` \n", "       flag is unset.\n", "    \"\"\"\n", "    file = os.popen(commands)\n", "    print (file.read().rstrip('\\n'))\n", "    exit_status = file.close()\n", "    if warn and exit_status != None:\n", "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n", "    return exit_status\n", "\n", "shell(\"\"\"\n", "ls requirements.txt >/dev/null 2>&1\n", "if [ ! $? = 0 ]; then\n", " rm -rf .tmp\n", " git clone https://github.com/cs187-2021/lab3-4.git .tmp\n", " mv .tmp/tests ./\n", " mv .tmp/requirements.txt ./\n", " rm -rf .tmp\n", "fi\n", "pip install -q -r requirements.txt\n", "\"\"\")"]}, {"cell_type": "code", "execution_count": null, "id": "2df704c6", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["# Initialize Otter\n", "import otter\n", "grader = otter.Notebook()"]}, {"cell_type": "raw", "id": "891854ed", "metadata": {"jupyter": {"source_hidden": true}}, "source": ["%%latex\n", "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\newcommand{\\softmax}{\\operatorname{softmax}}\n", "\\newcommand{\\Prob}{\\Pr}\n", "\\newcommand{\\given}{\\,|\\,}"]}, {"cell_type": "markdown", "id": "f635e46b", "metadata": {"jupyter": {"source_hidden": true}}, "source": ["$$\n", "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n", "\\renewcommand{\\Prob}{\\Pr}\n", "\\renewcommand{\\given}{\\,|\\,}\n", "$$"]}, {"cell_type": "markdown", "id": "12a589f3", "metadata": {"colab_type": "text", "id": "YhNwlK5J_wU_", "tags": ["remove_for_latex"]}, "source": ["# CS187\n", "## Lab 3-4 - Probabilistic parsing and parse disambiguation"]}, {"cell_type": "markdown", "id": "c85168c7", "metadata": {"colab_type": "text", "id": "w9cQ2kCv_zax"}, "source": ["Continuing our work on PCFG, you'll implement a probabilistic version of the CKY algorithm and apply it to a parse disambiguation task."]}, {"cell_type": "markdown", "id": "d734985d", "metadata": {}, "source": ["# Preparations {-}"]}, {"cell_type": "code", "execution_count": null, "id": "529f63f9", "metadata": {"colab": {}, "colab_type": "code", "deletable": false, "editable": false, "id": "d8kPmrmwB2U9"}, "outputs": [], "source": ["import copy\n", "import math\n", "import nltk\n", "import pandas as pd\n", "\n", "from collections import Counter\n", "from collections import defaultdict\n", "from pprint import pprint"]}, {"cell_type": "markdown", "id": "bd633dd5", "metadata": {"colab_type": "text", "id": "u6FOj0mSZp9j"}, "source": ["# Prepositional phrase attachment ambiguity\n", "\n", "In this lab, we'll set aside the rather limited world of arithmetic expressions, focusing on a common example of structural ambiguity in natural language called _prepositional phrase (PP) attachment_. A PP can modify both noun phrases and verb phrases, often creating ambiguity as to what constituent a PP should be attached to.\n", "\n", "Here's a small grammar that includes PPs as noun phrase modifiers, as verb phrase modifiers, and as verb arguments."]}, {"cell_type": "code", "execution_count": null, "id": "9a643ea7", "metadata": {}, "outputs": [], "source": ["probabilistic_grammar = nltk.PCFG.fromstring(\n", "    \"\"\"\n", "    S -> NP VP [1.0]\n", "\n", "    NP -> DT N [0.6]\n", "\n", "    N -> N PP [0.2]\n", "\n", "    VP -> TV NP [0.9] | DTV NP PP [0.1] \n", "\n", "    PP -> P NP [1.0]\n", "\n", "    DT ->  'a' [0.5] | 'the' [0.5]\n", "\n", "    N -> 'books' [0.2] | 'gifts' [0.2]\n", "    N -> 'table' [0.2] | 'book' [0.2] \n", "\n", "    NP -> 'Twain' [0.2] | 'Howells' [0.2]\n", "\n", "    DTV -> 'bought' [0.5] | 'put' [0.5]\n", "    TV ->  'bought' [0.5] | 'saw' [0.5]\n", "\n", "    P ->   'on' [0.3] | 'of' [0.4] | 'by' [0.1] | 'for' [0.2]\n", "    \"\"\"\n", ")"]}, {"cell_type": "markdown", "id": "41d27899", "metadata": {"colab_type": "text", "id": "cbJLgdNwqnES"}, "source": ["> In this grammar, `TV` stands for \"transitive verb\", a verb that takes a single NP argument, as in \"Twain bought it\", and `DTV` stands for \"ditransitive verb\", which takes two arguments, an NP and a PP, as in \"Twain bought it for him\".\n", "\n", "Notice that the probabilities of all the rules with the same left-hand side sum to 1.\n", "\n", "Consider the sentence"]}, {"cell_type": "code", "execution_count": null, "id": "217b8d11", "metadata": {"colab": {}, "colab_type": "code", "id": "951M07XfadFQ"}, "outputs": [], "source": ["example1 = \"Twain bought a book for Howells\""]}, {"cell_type": "markdown", "id": "2708048f", "metadata": {"colab_type": "text", "id": "cbJLgdNwqnES"}, "source": ["Take out a piece of paper and work with your lab partner to draw parse trees for the sentence according to this grammar. How many can you find?\n", "\n", "To verify, we'll parse the sentence using a parser provided by NLTK. The `InsideChartParser` returns all parses of a sentence according to a probabilistic grammar along with their probabilities. (It uses a more general algorithm than CKY, so doesn't require the grammar be in CNF.)\n", "\n", "Parsing this sentence with the above PCFG results in two possible parses, displaying PP attachment ambiguity:"]}, {"cell_type": "code", "execution_count": null, "id": "dcfb422a", "metadata": {"colab": {}, "colab_type": "code", "id": "T_2cjuU-qutW"}, "outputs": [], "source": ["parser = nltk.parse.InsideChartParser(probabilistic_grammar)\n", "possible_parses = list(parser.parse(example1.split()))\n", "\n", "for i, tree in enumerate(possible_parses):\n", "  print(f'Possible parse #{i+1} with probability {tree.prob():.3g}:\\n')\n", "  tree.pretty_print()"]}, {"cell_type": "markdown", "id": "bd3dcddd", "metadata": {"colab_type": "text", "deletable": false, "editable": false, "id": "pf3cO1wxlITu"}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** What is the more natural parsing, the one that leads to the preferred _reading_ of the sentence, the reading that you understand the sentence as expressing? Is it the most probable parse tree?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_pp1\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "id": "9e4e84ed", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "id": "75ed48f3", "metadata": {"colab_type": "text", "deletable": false, "editable": false, "id": "v7I1kdr-lftF"}, "source": ["<!-- END QUESTION -->\n", "\n", "Change some of the rule probabilities (try to change as few as possible) such that the other tree has higher probability.\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: attach_pp_to_np_1\n", "-->"]}, {"cell_type": "code", "execution_count": null, "id": "096f00d0", "metadata": {"colab": {}, "colab_type": "code", "id": "IdyQ5Qr3rchB"}, "outputs": [], "source": ["# TODO - define `probabilistic_grammar_reweighted`.\n", "probabilistic_grammar_reweighted = ..."]}, {"cell_type": "code", "execution_count": null, "id": "8f1c4edb", "metadata": {"colab": {}, "colab_type": "code", "id": "T_2cjuU-qutW"}, "outputs": [], "source": ["parser2 = nltk.parse.InsideChartParser(probabilistic_grammar_reweighted)\n", "possible_parses2 = list(parser2.parse(example1.split()))\n", "\n", "for i, tree in enumerate(possible_parses2):\n", "  print(f'Possible parse #{i+1} with probability {tree.prob():.3g}:\\n')\n", "  tree.pretty_print()"]}, {"cell_type": "markdown", "id": "2f65fc17", "metadata": {"colab_type": "text", "id": "RCCIbCcqm5h9"}, "source": ["Now we use the PCFG you defined to parse an only slightly different sentence."]}, {"cell_type": "code", "execution_count": null, "id": "f6c7d73a", "metadata": {"colab": {}, "colab_type": "code", "id": "OX7EIQOWnaQI"}, "outputs": [], "source": ["example2 = \"Twain bought a book by Howells\"\n", "\n", "possible_parses = list(parser2.parse(example2.split()))\n", "\n", "for i, tree in enumerate(possible_parses):\n", "  print('Possible parse #{} with probability {:.3g}:\\n'.format(i+1,tree.prob()))\n", "  tree.pretty_print()"]}, {"cell_type": "markdown", "id": "a993a766", "metadata": {"colab_type": "text", "deletable": false, "editable": false, "id": "JbM05kEStEO9"}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** Now what is the more natural parse for the sentence? Is it the most probable one? Can the PCFG be modified such that both sentences are parsed according to the natural readings for these sentences? Try to explain the problem.\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_attach_pp_to_np_2\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "id": "8ee50a54", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "id": "3e3aa550", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- END QUESTION -->\n", "\n", "# Conversion to Chomsky Normal Form\n", "\n", "The grammar (`probabilistic_grammar`) is almost in Chomsky Normal Form (which would make it suitable for the CKY algorithm). How many of its rules are not in CNF?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: non_cnf_rule_count\n", "-->"]}, {"cell_type": "code", "execution_count": null, "id": "955ac419", "metadata": {}, "outputs": [], "source": ["# TODO\n", "non_cnf_rule_count = ..."]}, {"cell_type": "code", "execution_count": null, "id": "63c5c44f", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"non_cnf_rule_count\")"]}, {"cell_type": "markdown", "id": "5e33153d", "metadata": {"deletable": false, "editable": false}, "source": ["Convert the grammar `probabilistic_grammar` by hand to a CNF grammar. Try to make as few changes to the grammar as possible.\n", "\n", "> **Important:** You should start with `probabilistic_grammar`, not `probabilistic_grammar_reweighted`.\n", "\n", "> **Hint:** As part of your solution, you should make use of a new nonterminal `NP_PP`. (Later tests will depend on this.)\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: cnf_conversion\n", "-->"]}, {"cell_type": "code", "execution_count": null, "id": "5981abae", "metadata": {}, "outputs": [], "source": ["# TODO - convert `probabilistic_grammar` to CNF. You should make\n", "#        use of a new nonterminal `NP_PP`.\n", "probabilistic_grammar_cnf = nltk.PCFG.fromstring(\n", "    ...\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "40f34f79", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"cnf_conversion\")"]}, {"cell_type": "markdown", "id": "a7760f33", "metadata": {"colab_type": "text", "id": "zTIhaV-2B9Ju"}, "source": ["# Probabilistic CKY\n", "\n", "You've been availing yourself of a probabilistic parsing algorithm in the NLTK package. It's time to see how such algorithms work. In lab 3-2 you worked with the CKY algorithm as a recognizer and its extension to a parser using backpointers. In the following section you will familiarize yourself with the probabilistic extension of the CKY parser, as presented by Jurafsky & Martin (Chapter 14), which returns the most probable parse (MPP) of a string according to a PCFG grammar."]}, {"cell_type": "markdown", "id": "67fc25a1", "metadata": {"colab_type": "text", "id": "zTIhaV-2B9Ju"}, "source": ["Now that we have a CNF grammar `probabilistic_grammar_cnf`, we can use the CKY algorithm to parse an example sentence. For reference, here is a pseudo-code version of the probabilistic CKY algorithm:\n", "\n", "```\n", " 1.  define cky-mpp(string = w1, ..., wN, grammar):\n", " 2.      for j in [1..N]:                     # each end string position\n", "\n", "             # handle rules of the form A -> w\n", " 3.          for all A where A -> wj in grammar:\n", " 4.              T[j-1, j, A] := Pr(A -> wj)\n", "\n", "             # handle rules of the form A -> B C\n", " 5.          for length in [2..j]:            # each subconstituent length\n", " 6.              i := j - length              # start string position\n", " 7.              for split in [i+1..j-1]      # each split point\n", " 8.                  for all A where \n", " 9.                          A -> B C in grammar\n", "10.                          and T[i, split, B] > 0\n", "11.                          and T[split, j, C] > 0:\n", "12.                      new_prob := Pr(A -> B C)\n", "13.                                  x table[i, split, B]\n", "14.                                  x table[split, j, C]\n", "15.                      if T[i, j, A] < new_prob\n", "16.                         then T[i, j, A] := new_prob\n", "17,                              back[i, j, A] := (split, B, C)\n", "18.      return (build_tree(back[0, N, S]), T[0, N, S]\n", "```"]}, {"cell_type": "markdown", "id": "5a494289", "metadata": {"colab_type": "text", "id": "nSf5W779GHEA"}, "source": ["This PCKY algorithm is almost identical to the CKY variant you used in lab 3-2, with only a few differences, namely:\n", "\n", "1. Table dimensions for a sentence of $N$ words:\n", "\n", "  * CKY: $(N+1)\\times(N+1)$\n", "  \n", "  * PCKY: $(N+1)\\times(N+1)\\times |\\cal{N}|$\n", "\n", "2. Table values:\n", "\n", "  * CKY: list of constituents\n", "  \n", "  * PCKY: probabilities, where `table[i, j, A]` is the maximum probability of nonterminal `A` covering words between string positions `i` and `j`\n", "\n", "3. Backpointers:\n", "\n", "  * CKY: mapping from nonterminals to set of all possible split positions and rules\n", "  \n", "  * PCKY: mapping from nonterminals to the single most probable split position and rule"]}, {"cell_type": "markdown", "id": "f95b9661", "metadata": {"colab_type": "text", "id": "z2GbGpe8TYO8"}, "source": ["Notice that the probabilities of all the rules with the same left-hand side sum to 1."]}, {"cell_type": "markdown", "id": "fffa88f5", "metadata": {"colab_type": "text", "id": "z2GbGpe8TYO8"}, "source": ["We will implement the required $(N+1)\\times(N+1)\\times|\\cal{N}|$ three-dimensional tables as $(N+1)\\times(N+1)$ two-dimensional tables, in which each cell will hold a dictionary mapping nonterminals to the required entry values. We will implement separate recognition and backpointer tables:\n", "\n", "* For the recognition table `table`, the entry values are the probabilities.\n", "* For the backpointers table `back`, the entry values are the appropriate backpointer `(split, B, C)`.\n", "\n", "As in lab 3-2, all the cells that need not be filled contain '---'. All other cells are initialized with an appropriate default dictionary. Run the following code to initialize the tables. (You don't need to go over it, we will look at a specific cell to better understand the content.)"]}, {"cell_type": "code", "execution_count": null, "id": "e076ab65", "metadata": {"colab": {}, "colab_type": "code", "id": "uWMlGE8uU9ii"}, "outputs": [], "source": ["def make_tables(words):\n", "    words = [\"\"] + words\n", "    N = len(words)\n", "\n", "    # initialize data in tables\n", "    table_data = [[\"---\" for i in range(N)] for j in range(N)]\n", "    back_data = [[\"---\" for i in range(N)] for j in range(N)]\n", "\n", "    # add in upper triangular elements\n", "    for i in range(N):\n", "        for j in range(N):\n", "            if i < j:\n", "                table_data[i][j] = defaultdict(float)\n", "                back_data[i][j] = defaultdict(lambda x: None)\n", "\n", "    # generate corresponding data frames\n", "    table = pd.DataFrame(table_data, columns=words, index=range(N))\n", "    table.columns = pd.MultiIndex.from_arrays([table.columns] + [range(N)])\n", "    back = pd.DataFrame(back_data, columns=words, index=list(range(N)))\n", "    back.columns = pd.MultiIndex.from_arrays([back.columns] + [range(N)])\n", "    return (table, back)\n", "    \n", "table, back = make_tables(example1.split())"]}, {"cell_type": "markdown", "id": "b6fe38c4", "metadata": {"colab_type": "text", "id": "rAlkdikWl9Jv"}, "source": ["Let's print out one of the tables:"]}, {"cell_type": "code", "execution_count": null, "id": "5c3e5da5", "metadata": {"colab": {}, "colab_type": "code", "id": "Zq4etqyPhTuK"}, "outputs": [], "source": ["table"]}, {"cell_type": "markdown", "id": "77dea006", "metadata": {"colab_type": "text", "id": "FtpC95x4ykI6"}, "source": ["We'll \"play parser\" and fill both tables for the first four values of `j` following the pseudo-code above. Below, you'll finish filling the tables for the remaining two values of `j` yourself. Make sure you understand what's going on in the cell below before continuing."]}, {"cell_type": "code", "execution_count": null, "id": "6a1bc869", "metadata": {"colab": {}, "colab_type": "code", "id": "M_TcjKmOy7Lk"}, "outputs": [], "source": ["                              # j = 1 (Twain)\n", "table.iloc[0,1]['NP'] = 0.2\n", "\n", "                              # j = 2 (bought)\n", "table.iloc[1,2]['DTV'] = 0.5\n", "table.iloc[1,2]['TV'] = 0.5\n", "                                # i = 0, split = 1: no changes\n", "\n", "                              # j = 3 (a)\n", "table.iloc[2,3]['DT'] = 0.5\n", "                                # i = 1, split = 2: no changes\n", "                                # i = 0, split = 1: no changes\n", "                                # i = 0, split = 2: no changes\n", "\n", "                              # j = 4 (book)\n", "table.iloc[3,4]['N'] = 0.2\n", "                                # i = 2, split = 3\n", "table.iloc[2,4]['NP'] = 0.06\n", "back.iloc[2,4]['NP'] = (3, 'DT', 'N')\n", "                                # i = 1, split = 2\n", "table.iloc[1,4]['VP'] = 0.027\n", "back.iloc[1,4]['VP'] = (2, 'TV', 'NP')\n", "                                # i = 1, split = 3: no changes\n", "                                # i = 0, split = 1\n", "table.iloc[0,4]['S'] = 0.0054\n", "back.iloc[0,4]['S'] = (1, 'NP', 'VP')\n", "                                # i = 0, split = 2: no changes\n", "                                # i = 0, split = 3: no changes"]}, {"cell_type": "markdown", "id": "b1aaf449", "metadata": {}, "source": ["We can see what progress has been made so far by examining the parser table."]}, {"cell_type": "code", "execution_count": null, "id": "3d886349", "metadata": {"colab": {}, "colab_type": "code", "id": "jWmR6Wy7JVB9"}, "outputs": [], "source": ["table"]}, {"cell_type": "markdown", "id": "a5b2bbee", "metadata": {"colab_type": "text", "deletable": false, "editable": false, "id": "B0yObI6lMceA"}, "source": ["Now it's time for you to finish filling the last two columns of the table.\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: pcky_fill_table\n", "points: 6\n", "-->"]}, {"cell_type": "code", "execution_count": null, "id": "60729080", "metadata": {"colab": {}, "colab_type": "code", "id": "pJNuecjzMjdX"}, "outputs": [], "source": ["# TODO - Continue filling in the tables as shown above.\n", "#        *DO NOT* round any results since hidden tests check for them."]}, {"cell_type": "code", "execution_count": null, "id": "67faaa1c", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"pcky_fill_table\")"]}, {"cell_type": "markdown", "id": "d9a6786f", "metadata": {}, "source": ["In addition to the table of probabilities, the algorithm also maintains the backpointer table."]}, {"cell_type": "code", "execution_count": null, "id": "fc93f84b", "metadata": {}, "outputs": [], "source": ["back"]}, {"cell_type": "markdown", "id": "51cd3b78", "metadata": {"deletable": false, "editable": false}, "source": ["Reconstruct the parse tree that the algorithm finds by chasing backpointers in this table, and enter it using parenthesized string notation in the next cell.\n", "\n", "> **Hint:** You'll definitely need pencil and paper for this. Start with the entry at 0,6 and work backwards from there.\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: best_parse\n", "-->"]}, {"cell_type": "code", "execution_count": null, "id": "df419711", "metadata": {}, "outputs": [], "source": ["# TODO\n", "best_parse = nltk.Tree.fromstring(\n", "    ...\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "4610cb96", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"best_parse\")"]}, {"cell_type": "markdown", "id": "9e74518d", "metadata": {"colab_type": "text", "id": "DRg92AYbM6P3"}, "source": ["Let us look at the probability of this tree, as cached in the [0,6] entry in the probability table:"]}, {"cell_type": "code", "execution_count": null, "id": "aba31b4b", "metadata": {"colab": {}, "colab_type": "code", "id": "9i2B96CJNDdl"}, "outputs": [], "source": ["table.iloc[0,6]"]}, {"cell_type": "markdown", "id": "e7159aa8", "metadata": {"colab_type": "text", "deletable": false, "editable": false, "id": "X446HJmWNFIX"}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** Compare the probability of this tree you just computed to the probabilities of the parses that the NLTK algorithm generated at the start of the lab, and explain the result.\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_pcky\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "id": "4da36686", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "id": "273ab67f", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- END QUESTION -->\n", "\n", "<!-- BEGIN QUESTION -->\n", "\n", "## Lab debrief \u2013 for consensus submission only\n", "\n", "**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on might include the following: \n", "\n", "* Was the lab too long or too short?\n", "* Were the readings appropriate for the lab? \n", "* Was it clear (at least after you completed the lab) what the points of the exercises were? \n", "* Are there additions or changes you think would make the lab better?\n", "\n", "but you should comment on whatever aspects you found especially positive or negative.\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_debrief\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "id": "d9886c54", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "id": "ce5165d0", "metadata": {}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "# End of Lab 3-4 {-}"]}, {"cell_type": "markdown", "id": "69601da4", "metadata": {"deletable": false, "editable": false}, "source": ["---\n", "\n", "To double-check your work, the cell below will rerun all of the autograder tests."]}, {"cell_type": "code", "execution_count": null, "id": "7789a6ac", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check_all()"]}], "metadata": {"colab": {"collapsed_sections": [], "name": "lab3-4.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}, "title": "CS187 Lab 3-4: Probabilistic parsing and parse disambiguation"}, "nbformat": 4, "nbformat_minor": 5}